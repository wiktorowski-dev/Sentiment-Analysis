{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Slice large file to plenty of smaller ones\n",
    "\n",
    "def slicer(input_path_to_file, output_file_path, output_lines):\n",
    "    out = []\n",
    "    i = 0\n",
    "    with open(input_path_to_file) as file:\n",
    "        for line in file:\n",
    "            out.append(line)\n",
    "            if len(out) > output_lines:\n",
    "                print(i)\n",
    "                with open(output_file_path.format(i), mode='a') as file2:\n",
    "                    file2.write(json.dumps(out))\n",
    "                    i += 1\n",
    "                    out = []\n",
    "        if len(out) != 0:\n",
    "            with open(output_file_path.format(i), mode='a') as file2:\n",
    "                file2.write(json.dumps(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(file_name):\n",
    "    print('\\t{}'.format(file_name))\n",
    "    # Process data\n",
    "    # Please notice that we using same name of variable to decrease memory usage\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    # Unpack json rows\n",
    "    file = [json.loads(x) for x in file]\n",
    "\n",
    "    file = pd.DataFrame(file)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    file = file.filter(['overall', 'reviewText'])\n",
    "\n",
    "    # Rename columns\n",
    "    file = file.rename(columns={'overall': 'sentiment', 'reviewText': 'text'})\n",
    "\n",
    "    # Transform 0-5 score, into positive/negative values\n",
    "\n",
    "    file = file[file.sentiment != 3]\n",
    "    file.sentiment = file.sentiment.apply(lambda x: 'positive' if x > 3 else 'negative')\n",
    "\n",
    "    # Remove noises\n",
    "    # Make all to string\n",
    "    file['text'] = file['text'].apply(str)\n",
    "\n",
    "    # We can remove some duplicates also at this point\n",
    "    file = file.drop_duplicates()\n",
    "\n",
    "    patterns = [\n",
    "        r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\",\n",
    "        r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\",\n",
    "        r\"(@[A-Za-z0-9_]+)\",\n",
    "        r\"(#[A-Za-z0-9_]+)\"]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        file['text'] = file['text'].apply(lambda x: re.sub(pattern, '', str(x)))\n",
    "\n",
    "    file.text = file.text.apply(lambda x: x if 0 < len(str(x).split()) < 100 else None)\n",
    "\n",
    "    # Remove nones\n",
    "    file = file.dropna()\n",
    "\n",
    "    # Write into csv file\n",
    "    file.to_csv(file_name, mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReSizerFiles(object):\n",
    "    def __init__(self):\n",
    "        super(ReSizerFiles, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def re_sizer(input_file_name: str, output_file_name: str,\n",
    "                 max_length_of_single_output: int, max_amount_output_of_files=float('inf'),\n",
    "                 total_length_of_output: int = float('inf'),\n",
    "                 auto_file_remover: bool = False, auto_detect_files_in_dir: bool = False, files_in_folder: int = None):\n",
    "\n",
    "        if not auto_detect_files_in_dir and not files_in_folder:\n",
    "            raise Exception(\"Declare files_in_folder or auto_detect_files_in_dir\")\n",
    "\n",
    "        if auto_detect_files_in_dir and not files_in_folder:\n",
    "            files_in_folder = [x for x in os.listdir(os.path.split(input_file_name)[0]) if\n",
    "                               os.path.split(input_file_name)[-1].replace(\"{\", '').replace(\"}\", '') in x\n",
    "                               and os.path.split(input_file_name)[-1].replace(\"{\", '').replace(\"}\", '') ==\n",
    "                               re.sub(r'\\d', '', x)].__len__()\n",
    "\n",
    "        files_name_lest_to_remove = [input_file_name.format(x) for x in range(files_in_folder)]\n",
    "\n",
    "        index_output_file = 0\n",
    "        df_out = pd.DataFrame()\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each file declared as input\n",
    "\n",
    "        for i in range(files_in_folder):\n",
    "\n",
    "            if files_in_folder == 0:\n",
    "                df.drop(df.index, inplace=True)\n",
    "                df_out.drop(df_out.index, inplace=True)\n",
    "                # Break after break in while loop\n",
    "                break\n",
    "\n",
    "            if i == 21:\n",
    "                print(123)\n",
    "            print(\"input_file_{}| output_file_{}\".format(i, index_output_file))\n",
    "\n",
    "            df = pd.read_csv(input_file_name.format(i))\n",
    "\n",
    "            length = df_out.__len__() + df.__len__()\n",
    "\n",
    "            # Check if file is enough large to save it\n",
    "            if length >= max_length_of_single_output or total_length_of_output <= max_length_of_single_output:\n",
    "                # Calculate left where to cut data frame to get equal value to declared one\n",
    "                to_cut = df.__len__() - (length - max_length_of_single_output)\n",
    "\n",
    "                if total_length_of_output < max_length_of_single_output:\n",
    "                    to_cut = total_length_of_output\n",
    "                    if to_cut == 0:\n",
    "                        df.drop(df.index, inplace=True)\n",
    "                        df_out.drop(df_out.index, inplace=True)\n",
    "                        break\n",
    "                    if length - max_length_of_single_output < 0:\n",
    "                        to_cut = total_length_of_output - df_out.__len__()\n",
    "\n",
    "                df_out = pd.concat([df_out, df[:to_cut]])\n",
    "\n",
    "                df_out.to_csv(output_file_name.format(index_output_file), mode='a', index=False)\n",
    "                total_length_of_output -= df_out.__len__()\n",
    "\n",
    "                # Overwrite file data frame and remove already saved data\n",
    "                df_out = pd.concat([df[to_cut:]])\n",
    "                df.drop(df.index, inplace=True)\n",
    "\n",
    "                if max_amount_output_of_files == index_output_file \\\n",
    "                        or total_length_of_output <= 0:\n",
    "                    df.drop(df.index, inplace=True)\n",
    "                    df_out.drop(df_out.index, inplace=True)\n",
    "                    # remove loop if get declared amount of file in output\n",
    "                    break\n",
    "\n",
    "                index_output_file += 1\n",
    "\n",
    "                # Loop for deal with larger file separated to few smaller one\n",
    "                while True:\n",
    "                    # Is size of df_positive is much larger than limit\n",
    "                    if df_out.__len__() > max_length_of_single_output:\n",
    "                        print(\"input_file_{}| output_file_{}\".format(i, index_output_file))\n",
    "                        if total_length_of_output < max_length_of_single_output:\n",
    "                            max_length_of_single_output = total_length_of_output\n",
    "\n",
    "                        df_out[:max_length_of_single_output].to_csv(output_file_name.format(index_output_file), mode='a',\n",
    "                                                                    index=False)\n",
    "\n",
    "                        total_length_of_output -= df_out[:max_length_of_single_output].__len__()\n",
    "\n",
    "                        df_out = df_out[max_length_of_single_output:]\n",
    "\n",
    "                        if max_amount_output_of_files <= index_output_file or \\\n",
    "                                total_length_of_output == 0:\n",
    "                            # remove loop if get declared amount of file in output\n",
    "                            files_in_folder = 0\n",
    "                            break\n",
    "\n",
    "                        index_output_file += 1\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                df_out = pd.concat([df_out, df])\n",
    "                df.drop(df.index, inplace=True)\n",
    "\n",
    "            if auto_file_remover:\n",
    "                os.remove(input_file_name.format(i))\n",
    "                files_name_lest_to_remove.remove(input_file_name.format(i))\n",
    "\n",
    "        if df_out.__len__() != 0:\n",
    "            df_out.to_csv(output_file_name.format(index_output_file), mode='a', index=False)\n",
    "\n",
    "        if auto_file_remover:\n",
    "            for file_to_remove in files_name_lest_to_remove:\n",
    "                os.remove(file_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_remover(input_file_name,\n",
    "                      output_file_name_neg,\n",
    "                      output_file_name_pos,\n",
    "                      files_in_folder,\n",
    "                      auto_remove_files=False):\n",
    "    # Drop duplicates\n",
    "    list_files_1 = [x for x in range(files_in_folder)]\n",
    "    list_files_2 = [x for x in range(files_in_folder)]\n",
    "\n",
    "    for i in list_files_1:\n",
    "        print('duplicate_remover {}'.format(i))\n",
    "        list_files_2.remove(i)\n",
    "        df1 = pd.read_csv(input_file_name.format(i))\n",
    "        # Remove duplicated rows in data frame\n",
    "        df1 = df1.drop_duplicates()\n",
    "\n",
    "        # Prepare positive list of sentences and filter them\n",
    "        pos1 = set(df1[df1.sentiment == 'positive']['text'])\n",
    "        pos1 = set(filter(lambda x: type(x) == str, pos1))\n",
    "\n",
    "        # Prepare negative list of sentences and filter them\n",
    "        neg1 = set(df1[df1.sentiment == 'negative']['text'])\n",
    "        neg1 = set(filter(lambda x: type(x) == str, neg1))\n",
    "\n",
    "        for i2 in list_files_2:\n",
    "            print('\\t{}'.format(i2))\n",
    "            df2 = pd.read_csv(input_file_name.format(i2))\n",
    "            df2 = df2.drop_duplicates()\n",
    "            pos2 = set(df2[df2.sentiment == 'positive']['text'])\n",
    "            neg2 = set(df2[df2.sentiment == 'negative']['text'])\n",
    "\n",
    "            # Remove duplicates sentences\n",
    "            pos1 = pos1 - pos2\n",
    "            neg1 = neg1 - neg2\n",
    "\n",
    "        # If you would like to don't use separate files use this lines below\n",
    "        # pos1 = [{'sentiment': 'positive', 'text': x} for x in pos1]\n",
    "        # neg1 = [{'sentiment': 'negative', 'text': x} for x in neg1]\n",
    "        # df1 = pd.DataFrame(pos1 + neg1)\n",
    "\n",
    "        df1 = pd.DataFrame(list(pos1))\n",
    "        df2 = pd.DataFrame(list(neg1))\n",
    "        df1.to_csv(output_file_name_pos.format(i), mode='a', index=False)\n",
    "        df2.to_csv(output_file_name_neg.format(i), mode='a', index=False)\n",
    "        if auto_remove_files:\n",
    "            os.remove(input_file_name.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_dir(input_file_path_with_name):\n",
    "    counted_files_in_folder = [x for x in os.listdir(os.path.split(input_file_path_with_name)[0]) if\n",
    "                               os.path.split(input_file_path_with_name)[-1].replace(\"{\", '').replace(\"}\", '') in x\n",
    "                               and os.path.split(input_file_path_with_name)[-1].replace(\"{\", '').replace(\"}\", '') ==\n",
    "                               re.sub(r'\\d', '', x)].__len__()\n",
    "\n",
    "    return counted_files_in_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_files(input_file, files: int):\n",
    "    out = []\n",
    "    out_sum = 0\n",
    "    for i in range(files):\n",
    "        df = pd.read_csv(input_file.format(i))\n",
    "        out.append(df.__len__())\n",
    "    for elem in out:\n",
    "        out_sum += elem\n",
    "    return out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Declare re sizer function\n",
    "    re_sizer = ReSizerFiles.re_sizer\n",
    "    #\n",
    "    tt = time.time()\n",
    "    #\n",
    "\n",
    "    # Slicer variables\n",
    "    input_path_to_large_file = r'D:\\s_a\\All_Amazon_Review.json'\n",
    "    output_file_path_post_slicer = r'D:\\s_a\\file_{}'\n",
    "\n",
    "    # .json to .csv formatter\n",
    "    cores = 4\n",
    "    input_file_path = r'D:\\s_a\\file_{}'\n",
    "\n",
    "    # Duplication remover variables\n",
    "    input_file_name = r'D:\\s_a\\file_large_{}'\n",
    "    output_file_name_neg = r'D:\\s_a\\file_large_n_{}'\n",
    "    output_file_name_pos = r'D:\\s_a\\file_large_p_{}'\n",
    "\n",
    "    # Re sizer variables\n",
    "    output_positive_files = r'D:\\s_a\\p_{}'\n",
    "    output_negative_files = r'D:\\s_a\\n_{}'\n",
    "\n",
    "    slicer(input_path_to_large_file, output_file_path_post_slicer, 1200000)\n",
    "\n",
    "    files_name = [input_file_path.format(x) for x in range(count_files_in_dir(input_file_path))]\n",
    "    with multiprocessing.Pool(cores) as p:\n",
    "        p.map(json_to_csv, files_name)\n",
    "\n",
    "    re_sizer(input_file_name=input_file_path,\n",
    "             output_file_name=input_file_name,\n",
    "             max_length_of_single_output=8000000,\n",
    "             auto_file_remover=True,\n",
    "             auto_detect_files_in_dir=True,\n",
    "             files_in_folder=None)\n",
    "\n",
    "    duplicate_remover(input_file_name,\n",
    "                      output_file_name_neg,\n",
    "                      output_file_name_pos,\n",
    "                      files_in_folder=count_files_in_dir(input_file_name),\n",
    "                      auto_remove_files=True)\n",
    "\n",
    "    re_sizer(input_file_name=output_file_name_neg,\n",
    "             output_file_name=output_negative_files,\n",
    "             max_length_of_single_output=1000000,\n",
    "             auto_file_remover=True,\n",
    "             auto_detect_files_in_dir=True,\n",
    "             files_in_folder=None)\n",
    "\n",
    "    re_sizer(input_file_name=output_file_name_pos,\n",
    "             output_file_name=output_positive_files,\n",
    "             max_length_of_single_output=1000000,\n",
    "             max_amount_output_of_files=count_files_in_dir(output_negative_files),\n",
    "             total_length_of_output=count_rows_in_files(output_negative_files,\n",
    "                                                        count_files_in_dir(output_negative_files)),\n",
    "             auto_file_remover=True,\n",
    "             auto_detect_files_in_dir=True,\n",
    "             files_in_folder=None)\n",
    "\n",
    "    o = time.time() - tt\n",
    "    print(o)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
